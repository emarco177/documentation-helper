{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MhwXdmHllHtV"
   },
   "source": [
    "# 🗺️ TavilyMap & TavilyExtract Tutorial\n",
    "\n",
    "> **📚 Part of the LangChain Course: Building AI Agents & RAG Apps**  \n",
    "> [🎓 Get the full course](https://www.udemy.com/course/langchain/?referralCode=D981B8213164A3EA91AC)\n",
    "\n",
    "\n",
    "This notebook demonstrates two powerful tools from Tavily AI:\n",
    "- **TavilyMap**: Automatically discovers and maps website structures\n",
    "- **TavilyExtract**: Extracts clean, structured content from web pages\n",
    "\n",
    "Perfect for documentation scraping, research, and content extraction! 🚀\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "znycq-qvlG1R"
   },
   "source": [
    "## 📦 Setup & Installation\n",
    "\n",
    "First, let's install the required packages and set up our environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4rOvqccCKFpU",
    "outputId": "744a63dc-7970-4f82-d5b7-11209d95e3f8"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install langchain-tavily certifi\n",
    "\n",
    "# For pretty printing and visualization\n",
    "!pip install rich pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rlIwFtP0KFpU",
    "outputId": "a0b47c17-8469-46b8-d0aa-54217cbb86ff"
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import ssl\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import certifi\n",
    "from langchain_tavily import TavilyExtract, TavilyMap\n",
    "from rich.console import Console\n",
    "from rich.panel import Panel\n",
    "\n",
    "# Configure SSL context\n",
    "ssl_context = ssl.create_default_context(cafile=certifi.where())\n",
    "os.environ[\"SSL_CERT_FILE\"] = certifi.where()\n",
    "os.environ[\"REQUESTS_CA_BUNDLE\"] = certifi.where()\n",
    "\n",
    "# Initialize rich console for pretty printing\n",
    "console = Console()\n",
    "\n",
    "\n",
    "print(\"✅ All imports successful!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EjtXG-Y3lVNi"
   },
   "source": [
    "## 🔑 API Key Setup\n",
    "\n",
    "You'll need a Tavily API key to use these tools. Get yours at [tavily.com](https://app.tavily.com/home?utm_campaign=eden_marco&utm_medium=socials&utm_source=linkedin).\n",
    "\n",
    "Set environment variable `TAVILY_API_KEY`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C3N8qUKSKFpU"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Set directly (uncomment and add your key)\n",
    "# tavily_api_key = \"your_tavily_api_key_here\"\n",
    "\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7hPhgHXkl937"
   },
   "source": [
    "## 🗺️ TavilyMap: Website Structure Discovery\n",
    "\n",
    "TavilyMap automatically discovers and maps website structures by crawling through links. It's perfect for:\n",
    "- Documentation sites\n",
    "- Blog archives\n",
    "- Knowledge bases\n",
    "- Any structured website\n",
    "\n",
    "### Key Parameters:\n",
    "- `max_depth`: How deep to crawl (default: 3)\n",
    "- `max_breadth`: How many links per page (default: 10)\n",
    "- `max_pages`: Maximum total pages to discover (default: 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DbAVGf-jKFpU",
    "outputId": "a1c161ef-1ec9-4849-beaa-8c67fa8a32ff"
   },
   "outputs": [],
   "source": [
    "# Initialize TavilyMap with custom settings\n",
    "tavily_map = TavilyMap(\n",
    "    max_depth=3,        # Crawl up to 3 levels deep\n",
    "    max_breadth=15,     # Follow up to 15 links per page\n",
    "    max_pages=50        # Limit to 50 total pages for demo\n",
    ")\n",
    "\n",
    "print(\"✅ TavilyMap initialized successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l1GJ0FPKmBhz"
   },
   "source": [
    "### 📊 Demo: Mapping a Documentation Site\n",
    "\n",
    "Let's map the structure of a popular documentation site. We'll use the FastAPI documentation as an example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 918
    },
    "id": "bh_ZAl7MKFpV",
    "outputId": "85982542-7b66-48af-fa4d-8f495b701281"
   },
   "outputs": [],
   "source": [
    "# Example website to map\n",
    "demo_url = \"https://python.langchain.com/docs/introduction/\"\n",
    "\n",
    "console.print(f\"🔍 Mapping website structure for: {demo_url}\", style=\"bold blue\")\n",
    "console.print(\"This may take a moment...\")\n",
    "\n",
    "# Map the website structure\n",
    "site_map = tavily_map.invoke(demo_url)\n",
    "\n",
    "# Display results\n",
    "urls = site_map.get('results', [])\n",
    "console.print(f\"\\n✅ Successfully mapped {len(urls)} URLs!\", style=\"bold green\")\n",
    "\n",
    "# Show first 10 URLs as examples\n",
    "console.print(\"\\n📋 First 50 discovered URLs:\", style=\"bold yellow\")\n",
    "for i, url in enumerate(urls[:50], 1):\n",
    "    console.print(f\"  {i:2d}. {url}\")\n",
    "\n",
    "if len(urls) > 10:\n",
    "    console.print(f\"  ... and {len(urls) - 50} more URLs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UHv-60zimbyH"
   },
   "source": [
    "## 🔍 TavilyExtract: Clean Content Extraction\n",
    "\n",
    "TavilyExtract takes URLs and returns clean, structured content without ads, navigation, or other noise. Perfect for:\n",
    "- Documentation processing\n",
    "- Content analysis\n",
    "- Research and data collection\n",
    "- Building knowledge bases\n",
    "\n",
    "### Key Features:\n",
    "- Removes HTML markup and navigation\n",
    "- Extracts main content only\n",
    "- Handles JavaScript-rendered content\n",
    "- Batch processing support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "5Vj1ZeaqKFpV",
    "outputId": "36e93bf4-29d5-43c9-fd44-11c6c8ce76a3"
   },
   "outputs": [],
   "source": [
    "# Initialize TavilyExtract\n",
    "tavily_extract = TavilyExtract()\n",
    "\n",
    "print(\"✅ TavilyExtract initialized successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QtLQklhamjiZ"
   },
   "source": [
    "### 📄 Demo: Extracting Content from URLs\n",
    "\n",
    "Let's extract clean content from some of the URLs we discovered earlier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 9443
    },
    "id": "hJ8NNHBDKFpV",
    "outputId": "b9e0c404-af90-48c8-e508-23dbd90a7e7a"
   },
   "outputs": [],
   "source": [
    "# Select a few interesting URLs for extraction\n",
    "sample_urls = [urls[15]]  # Take first 5 URLs\n",
    "console.print(f\"📚 Extracting content from {len(sample_urls)} URLs...\", style=\"bold blue\")\n",
    "\n",
    "# Extract content\n",
    "extraction_result = await tavily_extract.ainvoke(input={\"urls\": sample_urls})\n",
    "\n",
    "# Display results\n",
    "extracted_docs = extraction_result.get('results', [])\n",
    "console.print(f\"\\n✅ Successfully extracted {len(extracted_docs)} documents!\", style=\"bold green\")\n",
    "\n",
    "# Show summary of each extracted document\n",
    "for i, doc in enumerate(extracted_docs, 1):\n",
    "    url = doc.get('url', 'Unknown')\n",
    "    content = doc.get('raw_content', '')\n",
    "\n",
    "    # Create a panel for each document\n",
    "    panel_content = f\"\"\"URL: {url}\n",
    "Content Length: {len(content):,} characters\n",
    "Preview: {content}...\"\"\"\n",
    "\n",
    "    console.print(Panel(panel_content, title=f\"Document {i}\", border_style=\"blue\"))\n",
    "    print()  # Add spacing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxYWKCEDnvVU"
   },
   "source": [
    "### ⚡ Batch Processing Demo\n",
    "\n",
    "For larger datasets, we can process URLs in batches to optimize performance and handle rate limits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 167
    },
    "id": "9iqi-NRFKFpV",
    "outputId": "a8e88357-bf6c-4b6e-be52-e6e9d87aced7"
   },
   "outputs": [],
   "source": [
    "def chunk_urls(urls: List[str], chunk_size: int = 3) -> List[List[str]]:\n",
    "    \"\"\"Split URLs into chunks of specified size.\"\"\"\n",
    "    chunks = []\n",
    "    for i in range(0, len(urls), chunk_size):\n",
    "        chunk = urls[i:i + chunk_size]\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "async def extract_batch(urls: List[str], batch_num: int) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Extract documents from a batch of URLs.\"\"\"\n",
    "    try:\n",
    "        console.print(f\"🔄 Processing batch {batch_num} with {len(urls)} URLs\", style=\"blue\")\n",
    "        docs = await tavily_extract.ainvoke(input={\"urls\": urls})\n",
    "        results = docs.get('results', [])\n",
    "        console.print(f\"✅ Batch {batch_num} completed - extracted {len(results)} documents\", style=\"green\")\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        console.print(f\"❌ Batch {batch_num} failed: {e}\", style=\"red\")\n",
    "        return []\n",
    "\n",
    "# Process a larger set of URLs in batches\n",
    "url_batches = chunk_urls(urls[:9], chunk_size=3) # Take first 9 URLs for batch demo, split to batches of 3\n",
    "\n",
    "console.print(f\"📦 Processing 9 URLs in {len(url_batches)} batches\", style=\"bold yellow\")\n",
    "\n",
    "# Process batches concurrently\n",
    "tasks = [extract_batch(batch, i + 1) for i, batch in enumerate(url_batches)]\n",
    "batch_results = await asyncio.gather(*tasks)\n",
    "\n",
    "# Flatten results\n",
    "all_extracted = []\n",
    "for batch_result in batch_results:\n",
    "    all_extracted.extend(batch_result)\n",
    "\n",
    "console.print(f\"\\n🎉 Batch processing complete! Total documents extracted: {len(all_extracted)}\", style=\"bold green\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YkP6R-Sgn3dY"
   },
   "source": [
    "## 🎯 Real-World Use Cases\n",
    "\n",
    "Here are some practical applications of TavilyMap and TavilyExtract:\n",
    "\n",
    "### 1. Documentation Scraping\n",
    "- Map entire documentation sites\n",
    "- Extract clean content for search indexes\n",
    "- Build knowledge bases from existing docs\n",
    "\n",
    "### 2. Competitive Analysis\n",
    "- Map competitor websites\n",
    "- Extract product information\n",
    "- Monitor content changes\n",
    "\n",
    "### 3. Research & Content Collection\n",
    "- Gather information from multiple sources\n",
    "- Build datasets for analysis\n",
    "- Create content archives\n",
    "\n",
    "### 4. SEO & Site Analysis\n",
    "- Discover all pages on a site\n",
    "- Analyze content structure\n",
    "- Identify content gaps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DoyoDss4n8Tj"
   },
   "source": [
    "## 🎯 Conclusion\n",
    "\n",
    "This tutorial demonstrated the power of TavilyMap and TavilyExtract for automated web content discovery and extraction:\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **TavilyMap** is perfect for:\n",
    "   - Discovering website structures\n",
    "   - Finding all pages on a site\n",
    "   - Site auditing\n",
    "\n",
    "2. **TavilyExtract** excels at:\n",
    "   - Clean content extraction\n",
    "   - Removing HTML noise\n",
    "   - Batch processing\n",
    "   - Structured data collection\n",
    "\n",
    "3. **Combined** they enable:\n",
    "   - Complete documentation scraping\n",
    "   - Automated content pipelines\n",
    "   - Knowledge base creation\n",
    "   - Research automation\n",
    "\n",
    "### Next Steps:\n",
    "- Integrate with vector databases for semantic search\n",
    "- Add content filtering and classification\n",
    "- Build monitoring systems for content changes\n",
    "- Create automated reporting dashboards\n",
    "\n",
    "---\n",
    "\n",
    "**Happy scraping!** 🚀"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "znycq-qvlG1R"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "documentation-helper-H_4XlsCB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
